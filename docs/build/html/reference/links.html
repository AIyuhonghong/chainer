

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Standard Link implementations &mdash; Chainer 2.0.0b1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/modified_theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="Chainer 2.0.0b1 documentation" href="../index.html"/>
        <link rel="up" title="Chainer Reference Manual" href="index.html"/>
        <link rel="next" title="Optimizers" href="optimizers.html"/>
        <link rel="prev" title="Standard Function implementations" href="functions.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Chainer
          

          
          </a>

          
            
            
              <div class="version">
                2.0.0b1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial/index.html">Chainer Tutorial</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Chainer Reference Manual</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="core.html">Core functionalities</a></li>
<li class="toctree-l2"><a class="reference internal" href="util.html">Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="check.html">Assertion and Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="functions.html">Standard Function implementations</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Standard Link implementations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#learnable-connections">Learnable connections</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#bias">Bias</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bilinear">Bilinear</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convolution2d">Convolution2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#convolutionnd">ConvolutionND</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deconvolution2d">Deconvolution2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deconvolutionnd">DeconvolutionND</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dilatedconvolution2d">DilatedConvolution2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#embedid">EmbedID</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gru">GRU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#highway">Highway</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inception">Inception</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inceptionbn">InceptionBN</a></li>
<li class="toctree-l4"><a class="reference internal" href="#linear">Linear</a></li>
<li class="toctree-l4"><a class="reference internal" href="#lstm">LSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mlpconvolution2d">MLPConvolution2D</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nsteplstm">NStepLSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scale">Scale</a></li>
<li class="toctree-l4"><a class="reference internal" href="#statefulgru">StatefulGRU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#statefulpeepholelstm">StatefulPeepholeLSTM</a></li>
<li class="toctree-l4"><a class="reference internal" href="#statelesslstm">StatelessLSTM</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#activation-loss-normalization-functions-with-parameters">Activation/loss/normalization functions with parameters</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#batchnormalization">BatchNormalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#layernormalization">LayerNormalization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#binaryhierarchicalsoftmax">BinaryHierarchicalSoftmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="#blackout">BlackOut</a></li>
<li class="toctree-l4"><a class="reference internal" href="#crf1d">CRF1d</a></li>
<li class="toctree-l4"><a class="reference internal" href="#prelu">PReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#maxout">Maxout</a></li>
<li class="toctree-l4"><a class="reference internal" href="#negativesampling">NegativeSampling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#machine-learning-models">Machine learning models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#classifier">Classifier</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#pre-trained-models">Pre-trained models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#vgg16layers">VGG16Layers</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resnet50layers">ResNet50Layers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="optimizers.html">Optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="serializers.html">Serializers</a></li>
<li class="toctree-l2"><a class="reference internal" href="function_hooks.html">Function hooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="initializers.html">Weight Initializers</a></li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">Dataset examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="iterators.html">Iterator examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="extensions.html">Trainer extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="triggers.html">Trainer triggers</a></li>
<li class="toctree-l2"><a class="reference internal" href="caffe.html">Caffe Reference Model Support</a></li>
<li class="toctree-l2"><a class="reference internal" href="graph.html">Visualization of Computational Graph</a></li>
<li class="toctree-l2"><a class="reference internal" href="environment.html">Environment variables</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Chainer Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compatibility.html">API Compatibility Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tips.html">Tips and FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../comparison.html">Comparison with Other Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Chainer</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Chainer Reference Manual</a> &raquo;</li>
        
      <li>Standard Link implementations</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/reference/links.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-chainer.links">
<span id="standard-link-implementations"></span><h1>Standard Link implementations<a class="headerlink" href="#module-chainer.links" title="Permalink to this headline">Â¶</a></h1>
<p>Chainer provides many <a class="reference internal" href="core/link.html#chainer.Link" title="chainer.Link"><code class="xref py py-class docutils literal"><span class="pre">Link</span></code></a> implementations in the
<a class="reference internal" href="#module-chainer.links" title="chainer.links"><code class="xref py py-mod docutils literal"><span class="pre">chainer.links</span></code></a> package.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Some of the links are originally defined in the <a class="reference internal" href="functions.html#module-chainer.functions" title="chainer.functions"><code class="xref py py-mod docutils literal"><span class="pre">chainer.functions</span></code></a>
namespace. They are still left in the namespace for backward compatibility,
though it is strongly recommended to use them via the <a class="reference internal" href="#module-chainer.links" title="chainer.links"><code class="xref py py-mod docutils literal"><span class="pre">chainer.links</span></code></a>
package.</p>
</div>
<div class="section" id="learnable-connections">
<h2>Learnable connections<a class="headerlink" href="#learnable-connections" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="bias">
<h3>Bias<a class="headerlink" href="#bias" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Bias">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Bias</code><span class="sig-paren">(</span><em>axis=1</em>, <em>shape=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/bias.html#Bias"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Bias" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Broadcasted elementwise summation with learnable parameters.</p>
<p>Computes a elementwise summation as <a class="reference internal" href="functions.html#chainer.functions.bias" title="chainer.functions.bias"><code class="xref py py-func docutils literal"><span class="pre">bias()</span></code></a>
function does except that its second input is a learnable bias parameter
<span class="math">\(b\)</span> the link has.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â The first axis of the first input of
<a class="reference internal" href="functions.html#chainer.functions.bias" title="chainer.functions.bias"><code class="xref py py-func docutils literal"><span class="pre">bias()</span></code></a> function along which its second
input is applied.</li>
<li><strong>shape</strong> (<em>tuple of ints</em>) â Shape of the learnable bias parameter. If
<code class="docutils literal"><span class="pre">None</span></code>, this link does not have learnable parameters so an
explicit bias needs to be given to its <code class="docutils literal"><span class="pre">__call__</span></code> methodâs second
input.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">See <a class="reference internal" href="functions.html#chainer.functions.bias" title="chainer.functions.bias"><code class="xref py py-func docutils literal"><span class="pre">bias()</span></code></a> for details.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>b</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Bias parameter if <code class="docutils literal"><span class="pre">shape</span></code> is given. Otherwise,
no attributes.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="bilinear">
<h3>Bilinear<a class="headerlink" href="#bilinear" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Bilinear">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Bilinear</code><span class="sig-paren">(</span><em>left_size</em>, <em>right_size</em>, <em>out_size</em>, <em>nobias=False</em>, <em>initialW=None</em>, <em>initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/bilinear.html#Bilinear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Bilinear" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bilinear layer that performs tensor multiplication.</p>
<p>Bilinear is a primitive link that wraps the
<a class="reference internal" href="functions.html#chainer.functions.bilinear" title="chainer.functions.bilinear"><code class="xref py py-func docutils literal"><span class="pre">bilinear()</span></code></a> functions. It holds parameters <code class="docutils literal"><span class="pre">W</span></code>,
<code class="docutils literal"><span class="pre">V1</span></code>, <code class="docutils literal"><span class="pre">V2</span></code>, and <code class="docutils literal"><span class="pre">b</span></code> corresponding to the arguments of
<a class="reference internal" href="functions.html#chainer.functions.bilinear" title="chainer.functions.bilinear"><code class="xref py py-func docutils literal"><span class="pre">bilinear()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>left_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vector <span class="math">\(e^1\)</span> (<span class="math">\(J\)</span>)</li>
<li><strong>right_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vector <span class="math">\(e^2\)</span> (<span class="math">\(K\)</span>)</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of output vector <span class="math">\(y\)</span> (<span class="math">\(L\)</span>)</li>
<li><strong>nobias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, parameters <code class="docutils literal"><span class="pre">V1</span></code>, <code class="docutils literal"><span class="pre">V2</span></code>, and <code class="docutils literal"><span class="pre">b</span></code> are
omitted.</li>
<li><strong>initialW</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Initializer for <span class="math">\(W\)</span>.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.
Shape of the array must be <code class="docutils literal"><span class="pre">(left_size,</span> <span class="pre">right_size,</span> <span class="pre">out_size)</span></code>.</li>
<li><strong>initial_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a>) â Bias initializers.
It should be a 3-tuple of callables that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code>
or <code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
They initialize <span class="math">\(V^1\)</span>, <span class="math">\(V^2\)</span>,  and <span class="math">\(b\)</span>,
respectively.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is a tuple of <cite>numpy.ndarray</cite>, the arrays are used as initial
bias value.
Each element of this tuple must have the shapes of
<code class="docutils literal"><span class="pre">(left_size,</span> <span class="pre">output_size)</span></code>, <code class="docutils literal"><span class="pre">(right_size,</span> <span class="pre">output_size)</span></code>,
and <code class="docutils literal"><span class="pre">(output_size,)</span></code>, respectively.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">See <a class="reference internal" href="functions.html#chainer.functions.bilinear" title="chainer.functions.bilinear"><code class="xref py py-func docutils literal"><span class="pre">chainer.functions.bilinear()</span></code></a> for details.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Bilinear weight parameter.</li>
<li><strong>V1</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Linear weight parameter for the first argument.</li>
<li><strong>V2</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Linear weight parameter for the second
argument.</li>
<li><strong>b</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Bias parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="convolution2d">
<h3>Convolution2D<a class="headerlink" href="#convolution2d" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Convolution2D">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Convolution2D</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>ksize</em>, <em>stride=1</em>, <em>pad=0</em>, <em>nobias=False</em>, <em>initialW=None</em>, <em>initial_bias=None</em>, <em>deterministic=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/convolution_2d.html#Convolution2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Convolution2D" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Two-dimensional convolutional layer.</p>
<p>This link wraps the <a class="reference internal" href="functions.html#chainer.functions.convolution_2d" title="chainer.functions.convolution_2d"><code class="xref py py-func docutils literal"><span class="pre">convolution_2d()</span></code></a> function and
holds the filter weight and bias vector as parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of input arrays. If <code class="docutils literal"><span class="pre">None</span></code>,
parameter initialization will be deferred until the first forward
data pass at which time the size will be determined.</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of output arrays.</li>
<li><strong>ksize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Size of filters (a.k.a. kernels).
<code class="docutils literal"><span class="pre">ksize=k</span></code> and <code class="docutils literal"><span class="pre">ksize=(k,</span> <span class="pre">k)</span></code> are equivalent.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Stride of filter applications.
<code class="docutils literal"><span class="pre">stride=s</span></code> and <code class="docutils literal"><span class="pre">stride=(s,</span> <span class="pre">s)</span></code> are equivalent.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Spatial padding width for input arrays.
<code class="docutils literal"><span class="pre">pad=p</span></code> and <code class="docutils literal"><span class="pre">pad=(p,</span> <span class="pre">p)</span></code> are equivalent.</li>
<li><strong>nobias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, then this link does not use the bias term.</li>
<li><strong>initialW</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Weight initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.</li>
<li><strong>initial_bias</strong> (<em>1-D array</em>) â Initial bias value.
May also be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â The output of this link can be
non-deterministic when it uses cuDNN.
If this option is <code class="docutils literal"><span class="pre">True</span></code>, then it forces cuDNN to use
a deterministic algorithm. This option is only available for
cuDNN version &gt;= v4.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">See <a class="reference internal" href="functions.html#chainer.functions.convolution_2d" title="chainer.functions.convolution_2d"><code class="xref py py-func docutils literal"><span class="pre">chainer.functions.convolution_2d()</span></code></a> for the definition of
two-dimensional convolution.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter.</li>
<li><strong>b</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Bias parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="convolutionnd">
<h3>ConvolutionND<a class="headerlink" href="#convolutionnd" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.ConvolutionND">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">ConvolutionND</code><span class="sig-paren">(</span><em>ndim</em>, <em>in_channels</em>, <em>out_channels</em>, <em>ksize</em>, <em>stride=1</em>, <em>pad=0</em>, <em>nobias=False</em>, <em>initialW=None</em>, <em>initial_bias=None</em>, <em>cover_all=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/convolution_nd.html#ConvolutionND"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.ConvolutionND" title="Permalink to this definition">Â¶</a></dt>
<dd><p>N-dimensional convolution layer.</p>
<p>This link wraps the <a class="reference internal" href="functions.html#chainer.functions.convolution_nd" title="chainer.functions.convolution_nd"><code class="xref py py-func docutils literal"><span class="pre">convolution_nd()</span></code></a> function and
holds the filter weight and bias vector as parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>ndim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of spatial dimensions.</li>
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of input arrays.</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of output arrays.</li>
<li><strong>ksize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>tuple of ints</em>) â Size of filters (a.k.a. kernels).
<code class="docutils literal"><span class="pre">ksize=k</span></code> and <code class="docutils literal"><span class="pre">ksize=(k,</span> <span class="pre">k,</span> <span class="pre">...,</span> <span class="pre">k)</span></code> are equivalent.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>tuple of ints</em>) â Stride of filter application.
<code class="docutils literal"><span class="pre">stride=s</span></code> and <code class="docutils literal"><span class="pre">stride=(s,</span> <span class="pre">s,</span> <span class="pre">...,</span> <span class="pre">s)</span></code> are equivalent.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>tuple of ints</em>) â Spatial padding width for input arrays.
<code class="docutils literal"><span class="pre">pad=p</span></code> and <code class="docutils literal"><span class="pre">pad=(p,</span> <span class="pre">p,</span> <span class="pre">...,</span> <span class="pre">p)</span></code> are equivalent.</li>
<li><strong>nobias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, then this function does not use the bias.</li>
<li><strong>initialW</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Weight initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.</li>
<li><strong>initial_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Bias initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial bias value.</li>
<li><strong>cover_all</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, all spatial locations are convoluted
into some output pixels. It may make the output size larger.
<code class="docutils literal"><span class="pre">cover_all</span></code> needs to be <code class="docutils literal"><span class="pre">False</span></code> if you want to use cuDNN.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">See <a class="reference internal" href="functions.html#chainer.functions.convolution_nd" title="chainer.functions.convolution_nd"><code class="xref py py-func docutils literal"><span class="pre">convolution_nd()</span></code></a> for the definition of
N-dimensional convolution. See
<a class="reference internal" href="functions.html#chainer.functions.convolution_2d" title="chainer.functions.convolution_2d"><code class="xref py py-func docutils literal"><span class="pre">convolution_2d()</span></code></a> for the definition of
two-dimensional convolution.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter.</li>
<li><strong>b</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Bias parameter. If <code class="docutils literal"><span class="pre">initial_bias</span></code> is <code class="docutils literal"><span class="pre">None</span></code>,
set to <code class="docutils literal"><span class="pre">None</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="deconvolution2d">
<h3>Deconvolution2D<a class="headerlink" href="#deconvolution2d" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Deconvolution2D">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Deconvolution2D</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>ksize</em>, <em>stride=1</em>, <em>pad=0</em>, <em>nobias=False</em>, <em>outsize=None</em>, <em>initialW=None</em>, <em>initial_bias=None</em>, <em>deterministic=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/deconvolution_2d.html#Deconvolution2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Deconvolution2D" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Two dimensional deconvolution function.</p>
<p>This link wraps the <a class="reference internal" href="functions.html#chainer.functions.deconvolution_2d" title="chainer.functions.deconvolution_2d"><code class="xref py py-func docutils literal"><span class="pre">deconvolution_2d()</span></code></a> function
and holds the filter weight and bias vector as parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of input arrays. If <code class="docutils literal"><span class="pre">None</span></code>,
parameter initialization will be deferred until the first forward
data pass at which time the size will be determined.</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of output arrays.</li>
<li><strong>ksize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Size of filters (a.k.a. kernels).
<code class="docutils literal"><span class="pre">ksize=k</span></code> and <code class="docutils literal"><span class="pre">ksize=(k,</span> <span class="pre">k)</span></code> are equivalent.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Stride of filter applications.
<code class="docutils literal"><span class="pre">stride=s</span></code> and <code class="docutils literal"><span class="pre">stride=(s,</span> <span class="pre">s)</span></code> are equivalent.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Spatial padding width for input arrays.
<code class="docutils literal"><span class="pre">pad=p</span></code> and <code class="docutils literal"><span class="pre">pad=(p,</span> <span class="pre">p)</span></code> are equivalent.</li>
<li><strong>nobias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, then this function does not use the bias
term.</li>
<li><strong>outsize</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)"><em>tuple</em></a>) â Expected output size of deconvolutional operation.
It should be pair of height and width <span class="math">\((out_H, out_W)\)</span>.
Default value is <code class="docutils literal"><span class="pre">None</span></code> and the outsize is estimated by
input size, stride and pad.</li>
<li><strong>initialW</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Weight initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.</li>
<li><strong>initial_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Bias initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial bias value.</li>
<li><strong>deterministic</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â The output of this link can be
non-deterministic when it uses cuDNN.
If this option is <code class="docutils literal"><span class="pre">True</span></code>, then it forces cuDNN to use
a deterministic algorithm. This option is only available for
cuDNN version &gt;= v4.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The filter weight has four dimensions <span class="math">\((c_I, c_O, k_H, k_W)\)</span>
which indicate the number of input channels, output channels,
height and width of the kernels, respectively.
The filter weight is initialized with i.i.d. Gaussian random samples, each
of which has zero mean and deviation <span class="math">\(\sqrt{1/(c_I k_H k_W)}\)</span> by
default.</p>
<p>The bias vector is of size <span class="math">\(c_O\)</span>.
Its elements are initialized by <code class="docutils literal"><span class="pre">bias</span></code> argument.
If <code class="docutils literal"><span class="pre">nobias</span></code> argument is set to True, then this function does not hold
the bias parameter.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">See <a class="reference internal" href="functions.html#chainer.functions.deconvolution_2d" title="chainer.functions.deconvolution_2d"><code class="xref py py-func docutils literal"><span class="pre">chainer.functions.deconvolution_2d()</span></code></a> for the definition of
two-dimensional convolution.</p>
</div>
</dd></dl>

</div>
<div class="section" id="deconvolutionnd">
<h3>DeconvolutionND<a class="headerlink" href="#deconvolutionnd" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.DeconvolutionND">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">DeconvolutionND</code><span class="sig-paren">(</span><em>ndim</em>, <em>in_channels</em>, <em>out_channels</em>, <em>ksize</em>, <em>stride=1</em>, <em>pad=0</em>, <em>nobias=False</em>, <em>outsize=None</em>, <em>initialW=None</em>, <em>initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/deconvolution_nd.html#DeconvolutionND"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.DeconvolutionND" title="Permalink to this definition">Â¶</a></dt>
<dd><p>N-dimensional deconvolution function.</p>
<p>This link wraps <a class="reference internal" href="functions.html#chainer.functions.deconvolution_nd" title="chainer.functions.deconvolution_nd"><code class="xref py py-func docutils literal"><span class="pre">deconvolution_nd()</span></code></a> function and
holds the filter weight and bias vector as its parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>ndim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of spatial dimensions.</li>
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of input arrays.</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of output arrays.</li>
<li><strong>ksize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>tuple of ints</em>) â Size of filters (a.k.a. kernels).
<code class="docutils literal"><span class="pre">ksize=k</span></code> and <code class="docutils literal"><span class="pre">ksize=(k,</span> <span class="pre">k,</span> <span class="pre">...,</span> <span class="pre">k)</span></code> are equivalent.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>tuple of ints</em>) â Stride of filter application.
<code class="docutils literal"><span class="pre">stride=s</span></code> and <code class="docutils literal"><span class="pre">stride=(s,</span> <span class="pre">s,</span> <span class="pre">...,</span> <span class="pre">s)</span></code> are equivalent.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>tuple of ints</em>) â Spatial padding width for input arrays.
<code class="docutils literal"><span class="pre">pad=p</span></code> and <code class="docutils literal"><span class="pre">pad=(p,</span> <span class="pre">p,</span> <span class="pre">...,</span> <span class="pre">p)</span></code> are equivalent.</li>
<li><strong>nobias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, then this function does not use the bias.</li>
<li><strong>outsize</strong> (<em>tuple of ints</em>) â Expected output size of deconvolutional
operation. It should be a tuple of ints that represents the output
size of each dimension. Default value is <code class="docutils literal"><span class="pre">None</span></code> and the outsize
is estimated with input size, stride and pad.</li>
<li><strong>initialW</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Weight initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.</li>
<li><strong>initial_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Bias initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial bias value.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.deconvolution_nd" title="chainer.functions.deconvolution_nd"><code class="xref py py-func docutils literal"><span class="pre">deconvolution_nd()</span></code></a></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter.</li>
<li><strong>b</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Bias parameter. If <code class="docutils literal"><span class="pre">initial_bias</span></code> is <code class="docutils literal"><span class="pre">None</span></code>,
set to <code class="docutils literal"><span class="pre">None</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="dilatedconvolution2d">
<h3>DilatedConvolution2D<a class="headerlink" href="#dilatedconvolution2d" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.DilatedConvolution2D">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">DilatedConvolution2D</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>ksize</em>, <em>stride=1</em>, <em>pad=0</em>, <em>dilate=1</em>, <em>nobias=False</em>, <em>initialW=None</em>, <em>initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/dilated_convolution_2d.html#DilatedConvolution2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.DilatedConvolution2D" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Two-dimensional dilated convolutional layer.</p>
<p>This link wraps the <a class="reference internal" href="functions.html#chainer.functions.dilated_convolution_2d" title="chainer.functions.dilated_convolution_2d"><code class="xref py py-func docutils literal"><span class="pre">dilated_convolution_2d()</span></code></a>
function and holds the filter weight and bias vector as parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of input arrays. If <code class="docutils literal"><span class="pre">None</span></code>,
parameter initialization will be deferred until the first forward
data pass at which time the size will be determined.</li>
<li><strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of output arrays.</li>
<li><strong>ksize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Size of filters (a.k.a. kernels).
<code class="docutils literal"><span class="pre">ksize=k</span></code> and <code class="docutils literal"><span class="pre">ksize=(k,</span> <span class="pre">k)</span></code> are equivalent.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Stride of filter applications.
<code class="docutils literal"><span class="pre">stride=s</span></code> and <code class="docutils literal"><span class="pre">stride=(s,</span> <span class="pre">s)</span></code> are equivalent.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Spatial padding width for input arrays.
<code class="docutils literal"><span class="pre">pad=p</span></code> and <code class="docutils literal"><span class="pre">pad=(p,</span> <span class="pre">p)</span></code> are equivalent.</li>
<li><strong>dilate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Dilation factor of filter applications.
<code class="docutils literal"><span class="pre">dilate=d</span></code> and <code class="docutils literal"><span class="pre">dilate=(d,</span> <span class="pre">d)</span></code> are equivalent.</li>
<li><strong>nobias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, then this link does not use the bias term.</li>
<li><strong>initialW</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Weight initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.</li>
<li><strong>initial_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Bias initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial bias value.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">See <a class="reference internal" href="functions.html#chainer.functions.dilated_convolution_2d" title="chainer.functions.dilated_convolution_2d"><code class="xref py py-func docutils literal"><span class="pre">chainer.functions.dilated_convolution_2d()</span></code></a>
for the definition of two-dimensional dilated convolution.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter.</li>
<li><strong>b</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Bias parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="embedid">
<h3>EmbedID<a class="headerlink" href="#embedid" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.EmbedID">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">EmbedID</code><span class="sig-paren">(</span><em>in_size</em>, <em>out_size</em>, <em>initialW=None</em>, <em>ignore_label=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/embed_id.html#EmbedID"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.EmbedID" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Efficient linear layer for one-hot input.</p>
<p>This is a link that wraps the <a class="reference internal" href="functions.html#chainer.functions.embed_id" title="chainer.functions.embed_id"><code class="xref py py-func docutils literal"><span class="pre">embed_id()</span></code></a> function.
This link holds the ID (word) embedding matrix <code class="docutils literal"><span class="pre">W</span></code> as a parameter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of different identifiers (a.k.a. vocabulary
size).</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Size of embedding vector.</li>
<li><strong>initialW</strong> (<em>2-D array</em>) â Initial weight value. If <code class="docutils literal"><span class="pre">None</span></code>, then the
matrix is initialized from the standard normal distribution.
May also be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.</li>
<li><strong>ignore_label</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.6)"><em>None</em></a>) â If <code class="docutils literal"><span class="pre">ignore_label</span></code> is an int value,
<code class="docutils literal"><span class="pre">i</span></code>-th column of return value is filled with <code class="docutils literal"><span class="pre">0</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.embed_id" title="chainer.functions.embed_id"><code class="xref py py-func docutils literal"><span class="pre">chainer.functions.embed_id()</span></code></a></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Embedding parameter matrix.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="gru">
<h3>GRU<a class="headerlink" href="#gru" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.GRU">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">GRU</code><span class="sig-paren">(</span><em>n_units</em>, <em>n_inputs=None</em>, <em>init=None</em>, <em>inner_init=None</em>, <em>bias_init=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/gru.html#GRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.GRU" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Stateless Gated Recurrent Unit function (GRU).</p>
<p>GRU function has six parameters <span class="math">\(W_r\)</span>, <span class="math">\(W_z\)</span>, <span class="math">\(W\)</span>,
<span class="math">\(U_r\)</span>, <span class="math">\(U_z\)</span>, and <span class="math">\(U\)</span>. All these parameters are
<span class="math">\(n \times n\)</span> matrices, where <span class="math">\(n\)</span> is the dimension of
hidden vectors.</p>
<p>Given two inputs a previous hidden vector <span class="math">\(h\)</span> and an input vector
<span class="math">\(x\)</span>, GRU returns the next hidden vector <span class="math">\(h'\)</span> defined as</p>
<div class="math">
\[\begin{split}r &amp;=&amp; \sigma(W_r x + U_r h), \\
z &amp;=&amp; \sigma(W_z x + U_z h), \\
\bar{h} &amp;=&amp; \tanh(W x + U (r \odot h)), \\
h' &amp;=&amp; (1 - z) \odot h + z \odot \bar{h},\end{split}\]</div>
<p>where <span class="math">\(\sigma\)</span> is the sigmoid function, and <span class="math">\(\odot\)</span> is the
element-wise product.</p>
<p><a class="reference internal" href="#chainer.links.GRU" title="chainer.links.GRU"><code class="xref py py-class docutils literal"><span class="pre">GRU</span></code></a> does not hold the value of
hidden vector <span class="math">\(h\)</span>. So this is <em>stateless</em>.
Use <a class="reference internal" href="#chainer.links.StatefulGRU" title="chainer.links.StatefulGRU"><code class="xref py py-class docutils literal"><span class="pre">StatefulGRU</span></code></a> as a <em>stateful</em> GRU.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_units</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of hidden vector <span class="math">\(h\)</span>.</li>
<li><strong>n_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vector <span class="math">\(x\)</span>. If <code class="docutils literal"><span class="pre">None</span></code>,
it is set to the same value as <code class="docutils literal"><span class="pre">n_units</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>See:</dt>
<dd><ul class="first last simple">
<li><a class="reference external" href="http://www.aclweb.org/anthology/W14-4012">On the Properties of Neural Machine Translation: Encoder-Decoder
Approaches</a>
[Cho+, SSST2014].</li>
<li><a class="reference external" href="https://arxiv.org/abs/1412.3555">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
Modeling</a>
[Chung+NIPS2014 DLWorkshop].</li>
</ul>
</dd>
</dl>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#chainer.links.StatefulGRU" title="chainer.links.StatefulGRU"><code class="xref py py-class docutils literal"><span class="pre">StatefulGRU</span></code></a></p>
</div>
</dd></dl>

</div>
<div class="section" id="highway">
<h3>Highway<a class="headerlink" href="#highway" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Highway">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Highway</code><span class="sig-paren">(</span><em>in_out_size</em>, <em>nobias=False</em>, <em>activate=&lt;function relu&gt;</em>, <em>init_Wh=None</em>, <em>init_Wt=None</em>, <em>init_bh=None</em>, <em>init_bt=-1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/highway.html#Highway"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Highway" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Highway module.</p>
<p>In highway network, two gates are added to the ordinal non-linear
transformation (<span class="math">\(H(x) = activate(W_h x + b_h)\)</span>).
One gate is the transform gate <span class="math">\(T(x) = \sigma(W_t x + b_t)\)</span>, and the
other is the carry gate <span class="math">\(C(x)\)</span>.
For simplicity, the author defined <span class="math">\(C = 1 - T\)</span>.
Highway module returns <span class="math">\(y\)</span> defined as</p>
<div class="math">
\[y = activate(W_h x + b_h) \odot \sigma(W_t x + b_t) +
x \odot(1 - \sigma(W_t x + b_t))\]</div>
<p>The output array has the same spatial size as the input. In order to
satisfy this, <span class="math">\(W_h\)</span> and <span class="math">\(W_t\)</span> must be square matrices.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input and output vectors.</li>
<li><strong>nobias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, then this function does not use the bias.</li>
<li><strong>activate</strong> â Activation function of plain array. <span class="math">\(tanh\)</span> is also
available.</li>
<li><strong>init_Wh</strong> (<em>2-D array</em>) â Initial weight value of plain array.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used to
initialize the weight matrix.
May also be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.</li>
<li><strong>init_bh</strong> (<em>1-D array</em>) â Initial bias value of plain array. If <code class="docutils literal"><span class="pre">None</span></code>,
then this function uses it to initialize zero vector.
May also be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.</li>
<li><strong>init_Wt</strong> (<em>2-D array</em>) â Initial weight value of transform array.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used to
initialize the weight matrix.
May also be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.</li>
<li><strong>init_bt</strong> (<em>1-D array</em>) â Initial bias value of transform array.
Default value is -1 vector.
May also be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
Negative value is recommended by the author of the paper.
(e.g. -1, -3, â¦).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>See:</dt>
<dd><a class="reference external" href="https://arxiv.org/abs/1505.00387">Highway Networks</a>.</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="inception">
<h3>Inception<a class="headerlink" href="#inception" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Inception">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Inception</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out1</em>, <em>proj3</em>, <em>out3</em>, <em>proj5</em>, <em>out5</em>, <em>proj_pool</em>, <em>conv_init=None</em>, <em>bias_init=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/inception.html#Inception"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Inception" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Inception module of GoogLeNet.</p>
<p>It applies four different functions to the input array and concatenates
their outputs along the channel dimension. Three of them are 2D
convolutions of sizes 1x1, 3x3 and 5x5. Convolution paths of 3x3 and 5x5
sizes have 1x1 convolutions (called projections) ahead of them. The other
path consists of 1x1 convolution (projection) and 3x3 max pooling.</p>
<p>The output array has the same spatial size as the input. In order to
satisfy this, Inception module uses appropriate padding for each
convolution and pooling.</p>
<p>See: <a class="reference external" href="https://arxiv.org/abs/1409.4842">Going Deeper with Convolutions</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of input arrays.</li>
<li><strong>out1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Output size of 1x1 convolution path.</li>
<li><strong>proj3</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Projection size of 3x3 convolution path.</li>
<li><strong>out3</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Output size of 3x3 convolution path.</li>
<li><strong>proj5</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Projection size of 5x5 convolution path.</li>
<li><strong>out5</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Output size of 5x5 convolution path.</li>
<li><strong>proj_pool</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Projection size of max pooling path.</li>
<li><strong>conv_init</strong> â A callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
It is used for initialization of the convolution matrix weights.
Maybe be <code class="docutils literal"><span class="pre">None</span></code> to use default initialization.</li>
<li><strong>bias_init</strong> â A callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
It is used for initialization of the convolution bias weights.
Maybe be <code class="docutils literal"><span class="pre">None</span></code> to use default initialization.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="inceptionbn">
<h3>InceptionBN<a class="headerlink" href="#inceptionbn" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.InceptionBN">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">InceptionBN</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out1</em>, <em>proj3</em>, <em>out3</em>, <em>proj33</em>, <em>out33</em>, <em>pooltype</em>, <em>proj_pool=None</em>, <em>stride=1</em>, <em>conv_init=None</em>, <em>dtype=&lt;type 'numpy.float32'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/inceptionbn.html#InceptionBN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.InceptionBN" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Inception module of the new GoogLeNet with BatchNormalization.</p>
<p>This chain acts like <a class="reference internal" href="#chainer.links.Inception" title="chainer.links.Inception"><code class="xref py py-class docutils literal"><span class="pre">Inception</span></code></a>, while InceptionBN uses the
<a class="reference internal" href="#chainer.links.BatchNormalization" title="chainer.links.BatchNormalization"><code class="xref py py-class docutils literal"><span class="pre">BatchNormalization</span></code></a> on top of each convolution, the 5x5 convolution
path is replaced by two consecutive 3x3 convolution applications, and the
pooling method is configurable.</p>
<p>See: <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing     Internal Covariate Shift</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels of input arrays.</li>
<li><strong>out1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Output size of the 1x1 convolution path.</li>
<li><strong>proj3</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Projection size of the single 3x3 convolution path.</li>
<li><strong>out3</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Output size of the single 3x3 convolution path.</li>
<li><strong>proj33</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Projection size of the double 3x3 convolutions path.</li>
<li><strong>out33</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Output size of the double 3x3 convolutions path.</li>
<li><strong>pooltype</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) â Pooling type. It must be either <code class="docutils literal"><span class="pre">'max'</span></code> or <code class="docutils literal"><span class="pre">'avg'</span></code>.</li>
<li><strong>proj_pool</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, do projection in the pooling path.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Stride parameter of the last convolution of each path.</li>
<li><strong>conv_init</strong> â A callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
It is used for initialization of the convolution matrix weights.
Maybe be <code class="docutils literal"><span class="pre">None</span></code> to use default initialization.</li>
<li><strong>dtype</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html#numpy.dtype" title="(in NumPy v1.13)"><em>numpy.dtype</em></a>) â Type to use in
<code class="docutils literal"><span class="pre">~batch_normalization.BatchNormalization</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#chainer.links.Inception" title="chainer.links.Inception"><code class="xref py py-class docutils literal"><span class="pre">Inception</span></code></a></p>
</div>
</dd></dl>

</div>
<div class="section" id="linear">
<h3>Linear<a class="headerlink" href="#linear" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Linear">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Linear</code><span class="sig-paren">(</span><em>in_size</em>, <em>out_size</em>, <em>nobias=False</em>, <em>initialW=None</em>, <em>initial_bias=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/linear.html#Linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Linear" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Linear layer (a.k.a. fully-connected layer).</p>
<p>This is a link that wraps the <a class="reference internal" href="functions.html#chainer.functions.linear" title="chainer.functions.linear"><code class="xref py py-func docutils literal"><span class="pre">linear()</span></code></a> function,
and holds a weight matrix <code class="docutils literal"><span class="pre">W</span></code> and optionally a bias vector <code class="docutils literal"><span class="pre">b</span></code> as
parameters.</p>
<p>The weight matrix <code class="docutils literal"><span class="pre">W</span></code> is initialized with i.i.d. Gaussian samples, each
of which has zero mean and deviation <span class="math">\(\sqrt{1/\text{in_size}}\)</span>. The
bias vector <code class="docutils literal"><span class="pre">b</span></code> is of size <code class="docutils literal"><span class="pre">out_size</span></code>. Each element is initialized with
the <code class="docutils literal"><span class="pre">bias</span></code> value. If <code class="docutils literal"><span class="pre">nobias</span></code> argument is set to True, then this link
does not hold a bias vector.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vectors. If <code class="docutils literal"><span class="pre">None</span></code>, parameter
initialization will be deferred until the first forward data pass
at which time the size will be determined.</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of output vectors.</li>
<li><strong>nobias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, then this function does not use the bias.</li>
<li><strong>initialW</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Weight initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.</li>
<li><strong>initial_bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#callable" title="(in Python v3.6)"><em>callable</em></a>) â Bias initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial bias value.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.linear" title="chainer.functions.linear"><code class="xref py py-func docutils literal"><span class="pre">linear()</span></code></a></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter.</li>
<li><strong>b</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Bias parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="lstm">
<h3>LSTM<a class="headerlink" href="#lstm" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.LSTM">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">LSTM</code><span class="sig-paren">(</span><em>in_size</em>, <em>out_size</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/lstm.html#LSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.LSTM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fully-connected LSTM layer.</p>
<p>This is a fully-connected LSTM layer as a chain. Unlike the
<a class="reference internal" href="functions.html#chainer.functions.lstm" title="chainer.functions.lstm"><code class="xref py py-func docutils literal"><span class="pre">lstm()</span></code></a> function, which is defined as a stateless
activation function, this chain holds upward and lateral connections as
child links.</p>
<p>It also maintains <em>states</em>, including the cell state and the output
at the previous time step. Therefore, it can be used as a <em>stateful LSTM</em>.</p>
<p>This link supports variable length inputs. The mini-batch size of the
current input must be equal to or smaller than that of the previous one.
The mini-batch size of <code class="docutils literal"><span class="pre">c</span></code> and <code class="docutils literal"><span class="pre">h</span></code> is determined as that of the first
input <code class="docutils literal"><span class="pre">x</span></code>.
When mini-batch size of <code class="docutils literal"><span class="pre">i</span></code>-th input is smaller than that of the previous
input, this link only updates <code class="docutils literal"><span class="pre">c[0:len(x)]</span></code> and <code class="docutils literal"><span class="pre">h[0:len(x)]</span></code> and
doesnât change the rest of <code class="docutils literal"><span class="pre">c</span></code> and <code class="docutils literal"><span class="pre">h</span></code>.
So, please sort input sequences in descending order of lengths before
applying the function.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vectors. If <code class="docutils literal"><span class="pre">None</span></code>, parameter
initialization will be deferred until the first forward data pass
at which time the size will be determined.</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimensionality of output vectors.</li>
<li><strong>lateral_init</strong> â A callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
It is used for initialization of the lateral connections.
Maybe be <code class="docutils literal"><span class="pre">None</span></code> to use default initialization.</li>
<li><strong>upward_init</strong> â A callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
It is used for initialization of the upward connections.
Maybe be <code class="docutils literal"><span class="pre">None</span></code> to use default initialization.</li>
<li><strong>bias_init</strong> â A callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value
It is used for initialization of the biases of cell input,
input gate and output gate.and gates of the upward connection.
Maybe a scalar, in that case, the bias is
initialized by this value.
Maybe be <code class="docutils literal"><span class="pre">None</span></code> to use default initialization.</li>
<li><strong>forget_bias_init</strong> â A callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value
It is used for initialization of the biases of the forget gate of
the upward connection.
Maybe a scalar, in that case, the bias is
initialized by this value.
Maybe be <code class="docutils literal"><span class="pre">None</span></code> to use default initialization.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>upward</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>Linear</em></a>) â Linear layer of upward connections.</li>
<li><strong>lateral</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>Linear</em></a>) â Linear layer of lateral connections.</li>
<li><strong>c</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Cell states of LSTM units.</li>
<li><strong>h</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Output at the previous time step.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="chainer.links.LSTM.reset_state">
<code class="descname">reset_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/lstm.html#LSTM.reset_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.LSTM.reset_state" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Resets the internal state.</p>
<p>It sets <code class="docutils literal"><span class="pre">None</span></code> to the <code class="xref py py-attr docutils literal"><span class="pre">c</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">h</span></code> attributes.</p>
</dd></dl>

<dl class="method">
<dt id="chainer.links.LSTM.set_state">
<code class="descname">set_state</code><span class="sig-paren">(</span><em>c</em>, <em>h</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/lstm.html#LSTM.set_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.LSTM.set_state" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Sets the internal state.</p>
<p>It sets the <code class="xref py py-attr docutils literal"><span class="pre">c</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">h</span></code> attributes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>c</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â A new cell states of LSTM units.</li>
<li><strong>h</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â A new output at the previous time step.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="mlpconvolution2d">
<h3>MLPConvolution2D<a class="headerlink" href="#mlpconvolution2d" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.MLPConvolution2D">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">MLPConvolution2D</code><span class="sig-paren">(</span><em>in_channels</em>, <em>out_channels</em>, <em>ksize</em>, <em>stride=1</em>, <em>pad=0</em>, <em>activation=&lt;function relu&gt;</em>, <em>wscale=1</em>, <em>conv_init=None</em>, <em>bias_init=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/mlp_convolution_2d.html#MLPConvolution2D"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.MLPConvolution2D" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Two-dimensional MLP convolution layer of Network in Network.</p>
<p>This is an âmlpconvâ layer from the Network in Network paper. This layer
is a two-dimensional convolution layer followed by 1x1 convolution layers
and interleaved activation functions.</p>
<p>Note that it does not apply the activation function to the output of the
last 1x1 convolution layer.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.6)"><em>None</em></a>) â Number of channels of input arrays.
If <code class="docutils literal"><span class="pre">None</span></code>, parameter initialization will be deferred until the
first forward data pass at which time the size will be determined.</li>
<li><strong>out_channels</strong> (<em>tuple of ints</em>) â Tuple of number of channels. The i-th
integer indicates the number of filters of the i-th convolution.</li>
<li><strong>ksize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Size of filters (a.k.a. kernels) of the
first convolution layer. <code class="docutils literal"><span class="pre">ksize=k</span></code> and <code class="docutils literal"><span class="pre">ksize=(k,</span> <span class="pre">k)</span></code> are
equivalent.</li>
<li><strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Stride of filter applications at the
first convolution layer. <code class="docutils literal"><span class="pre">stride=s</span></code> and <code class="docutils literal"><span class="pre">stride=(s,</span> <span class="pre">s)</span></code> are
equivalent.</li>
<li><strong>pad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>pair of ints</em>) â Spatial padding width for input arrays at
the first convolution layer. <code class="docutils literal"><span class="pre">pad=p</span></code> and <code class="docutils literal"><span class="pre">pad=(p,</span> <span class="pre">p)</span></code> are
equivalent.</li>
<li><strong>activation</strong> (<a class="reference internal" href="function_hooks.html#module-chainer.function" title="chainer.function"><em>function</em></a>) â Activation function for internal hidden units.
Note that this function is not applied to the output of this link.</li>
<li><strong>conv_init</strong> â An initializer of weight matrices
passed to the convolution layers.</li>
<li><strong>bias_init</strong> â An initializer of bias vectors
passed to the convolution layers.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>See: <a class="reference external" href="https://arxiv.org/abs/1312.4400v3">Network in Network</a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>activation</strong> (<a class="reference internal" href="function_hooks.html#module-chainer.function" title="chainer.function"><em>function</em></a>) â Activation function.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="nsteplstm">
<h3>NStepLSTM<a class="headerlink" href="#nsteplstm" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.NStepLSTM">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">NStepLSTM</code><span class="sig-paren">(</span><em>n_layers</em>, <em>in_size</em>, <em>out_size</em>, <em>dropout</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/n_step_lstm.html#NStepLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.NStepLSTM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Stacked LSTM for sequnces.</p>
<p>This link is stacked version of LSTM for sequences. It calculates hidden
and cell states of all layer at end-of-string, and all hidden states of
the last layer for each time.</p>
<p>Unlike <a class="reference internal" href="functions.html#chainer.functions.n_step_lstm" title="chainer.functions.n_step_lstm"><code class="xref py py-func docutils literal"><span class="pre">chainer.functions.n_step_lstm()</span></code></a>, this function automatically
sort inputs in descending order by length, and transpose the seuqnece.
Users just need to call the link with a list of <a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><code class="xref py py-class docutils literal"><span class="pre">chainer.Variable</span></code></a>
holding sequences.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of layers.</li>
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimensionality of input vectors.</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimensionality of hidden states and output vectors.</li>
<li><strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Dropout ratio.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.n_step_lstm" title="chainer.functions.n_step_lstm"><code class="xref py py-func docutils literal"><span class="pre">chainer.functions.n_step_lstm()</span></code></a></p>
</div>
</dd></dl>

</div>
<div class="section" id="scale">
<h3>Scale<a class="headerlink" href="#scale" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Scale">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Scale</code><span class="sig-paren">(</span><em>axis=1</em>, <em>W_shape=None</em>, <em>bias_term=False</em>, <em>bias_shape=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/scale.html#Scale"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Scale" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Broadcasted elementwise product with learnable parameters.</p>
<p>Computes a elementwise product as <a class="reference internal" href="functions.html#chainer.functions.scale" title="chainer.functions.scale"><code class="xref py py-func docutils literal"><span class="pre">scale()</span></code></a>
function does except that its second input is a learnable weight parameter
<span class="math">\(W\)</span> the link has.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>axis</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â The first axis of the first input of
<a class="reference internal" href="functions.html#chainer.functions.scale" title="chainer.functions.scale"><code class="xref py py-func docutils literal"><span class="pre">scale()</span></code></a> function along which its second
input is applied.</li>
<li><strong>W_shape</strong> (<em>tuple of ints</em>) â Shape of learnable weight parameter. If
<code class="docutils literal"><span class="pre">None</span></code>, this link does not have learnable weight parameter so an
explicit weight needs to be given to its <code class="docutils literal"><span class="pre">__call__</span></code> methodâs
second input.</li>
<li><strong>bias_term</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â Whether to also learn a bias (equivalent to Scale
link + Bias link).</li>
<li><strong>bias_shape</strong> (<em>tuple of ints</em>) â Shape of learnable bias. If <code class="docutils literal"><span class="pre">W_shape</span></code> is
<code class="docutils literal"><span class="pre">None</span></code>, this should be given to determine the shape. Otherwise,
the bias has the same shape <code class="docutils literal"><span class="pre">W_shape</span></code> with the weight parameter
and <code class="docutils literal"><span class="pre">bias_shape</span></code> is ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">See <a class="reference internal" href="functions.html#chainer.functions.scale" title="chainer.functions.scale"><code class="xref py py-func docutils literal"><span class="pre">scale()</span></code></a> for details.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter if <code class="docutils literal"><span class="pre">W_shape</span></code> is given.
Otherwise, no W attribute.</li>
<li><a class="reference internal" href="functions.html#chainer.functions.bias" title="chainer.functions.bias"><strong>bias</strong></a> (<a class="reference internal" href="#chainer.links.Bias" title="chainer.links.Bias"><em>Bias</em></a>) â Bias term if <code class="docutils literal"><span class="pre">bias_term</span></code> is <code class="docutils literal"><span class="pre">True</span></code>.
Otherwise, no bias attribute.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="statefulgru">
<h3>StatefulGRU<a class="headerlink" href="#statefulgru" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.StatefulGRU">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">StatefulGRU</code><span class="sig-paren">(</span><em>in_size</em>, <em>out_size</em>, <em>init=None</em>, <em>inner_init=None</em>, <em>bias_init=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/gru.html#StatefulGRU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.StatefulGRU" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Stateful Gated Recurrent Unit function (GRU).</p>
<p>Stateful GRU function has six parameters <span class="math">\(W_r\)</span>, <span class="math">\(W_z\)</span>,
<span class="math">\(W\)</span>, <span class="math">\(U_r\)</span>, <span class="math">\(U_z\)</span>, and <span class="math">\(U\)</span>.
All these parameters are <span class="math">\(n \times n\)</span> matrices,
where <span class="math">\(n\)</span> is the dimension of hidden vectors.</p>
<p>Given input vector <span class="math">\(x\)</span>, Stateful GRU returns the next
hidden vector <span class="math">\(h'\)</span> defined as</p>
<div class="math">
\[\begin{split}r &amp;=&amp; \sigma(W_r x + U_r h), \\
z &amp;=&amp; \sigma(W_z x + U_z h), \\
\bar{h} &amp;=&amp; \tanh(W x + U (r \odot h)), \\
h' &amp;=&amp; (1 - z) \odot h + z \odot \bar{h},\end{split}\]</div>
<p>where <span class="math">\(h\)</span> is current hidden vector.</p>
<p>As the name indicates, <a class="reference internal" href="#chainer.links.StatefulGRU" title="chainer.links.StatefulGRU"><code class="xref py py-class docutils literal"><span class="pre">StatefulGRU</span></code></a> is <em>stateful</em>,
meaning that it also holds the next hidden vector <cite>hâ</cite> as a state.
Use <a class="reference internal" href="#chainer.links.GRU" title="chainer.links.GRU"><code class="xref py py-class docutils literal"><span class="pre">GRU</span></code></a> as a stateless version of GRU.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vector <span class="math">\(x\)</span>.</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of hidden vector <span class="math">\(h\)</span>.</li>
<li><strong>init</strong> â Initializer for GRUâs input units (<span class="math">\(W\)</span>).
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.</li>
<li><strong>inner_init</strong> â Initializer for the GRUâs inner
recurrent units (<span class="math">\(U\)</span>).
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If it is <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial
weight value.</li>
<li><strong>bias_init</strong> â Bias initializer.
It should be a callable that takes <code class="docutils literal"><span class="pre">numpy.ndarray</span></code> or
<code class="docutils literal"><span class="pre">cupy.ndarray</span></code> and edits its value.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used.
If it is <cite>numpy.ndarray</cite>, the array is used as initial bias value.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>h</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Hidden vector that indicates the state of
<a class="reference internal" href="#chainer.links.StatefulGRU" title="chainer.links.StatefulGRU"><code class="xref py py-class docutils literal"><span class="pre">StatefulGRU</span></code></a>.</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><code class="xref py py-class docutils literal"><span class="pre">GRU</span></code></p>
</div>
</dd></dl>

</div>
<div class="section" id="statefulpeepholelstm">
<h3>StatefulPeepholeLSTM<a class="headerlink" href="#statefulpeepholelstm" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.StatefulPeepholeLSTM">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">StatefulPeepholeLSTM</code><span class="sig-paren">(</span><em>in_size</em>, <em>out_size</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/peephole.html#StatefulPeepholeLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.StatefulPeepholeLSTM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fully-connected LSTM layer with peephole connections.</p>
<p>This is a fully-connected LSTM layer with peephole connections as a chain.
Unlike the <a class="reference internal" href="#chainer.links.LSTM" title="chainer.links.LSTM"><code class="xref py py-class docutils literal"><span class="pre">LSTM</span></code></a> link, this chain holds <code class="docutils literal"><span class="pre">peep_i</span></code>,
<code class="docutils literal"><span class="pre">peep_f</span></code> and <code class="docutils literal"><span class="pre">peep_o</span></code> as child links besides <code class="docutils literal"><span class="pre">upward</span></code> and
<code class="docutils literal"><span class="pre">lateral</span></code>.</p>
<p>Given a input vector <span class="math">\(x\)</span>, Peephole returns the next hidden vector
<span class="math">\(h'\)</span> defined as</p>
<div class="math">
\[\begin{split}a &amp;=&amp; \tanh(upward x + lateral h), \\
i &amp;=&amp; \sigma(upward x + lateral h + peep_i c), \\
f &amp;=&amp; \sigma(upward x + lateral h + peep_f c), \\
c' &amp;=&amp; a \odot i + f \odot c, \\
o &amp;=&amp; \sigma(upward x + lateral h + peep_o c'), \\
h' &amp;=&amp; o \tanh(c'),\end{split}\]</div>
<p>where <span class="math">\(\sigma\)</span> is the sigmoid function, <span class="math">\(\odot\)</span> is the
element-wise product, <span class="math">\(c\)</span> is the current cell state, <span class="math">\(c'\)</span>
is the next cell state and <span class="math">\(h\)</span> is the current hidden vector.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of the input vector <span class="math">\(x\)</span>.</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of the hidden vector <span class="math">\(h\)</span>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>upward</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>Linear</em></a>) â Linear layer of upward connections.</li>
<li><strong>lateral</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>Linear</em></a>) â Linear layer of lateral connections.</li>
<li><strong>peep_i</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>Linear</em></a>) â Linear layer of peephole connections
to the input gate.</li>
<li><strong>peep_f</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>Linear</em></a>) â Linear layer of peephole connections
to the forget gate.</li>
<li><strong>peep_o</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>Linear</em></a>) â Linear layer of peephole connections
to the output gate.</li>
<li><strong>c</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Cell states of LSTM units.</li>
<li><strong>h</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Output at the current time step.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="chainer.links.StatefulPeepholeLSTM.reset_state">
<code class="descname">reset_state</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/peephole.html#StatefulPeepholeLSTM.reset_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.StatefulPeepholeLSTM.reset_state" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Resets the internal states.</p>
<p>It sets <code class="docutils literal"><span class="pre">None</span></code> to the <code class="xref py py-attr docutils literal"><span class="pre">c</span></code> and <code class="xref py py-attr docutils literal"><span class="pre">h</span></code> attributes.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="statelesslstm">
<h3>StatelessLSTM<a class="headerlink" href="#statelesslstm" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.StatelessLSTM">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">StatelessLSTM</code><span class="sig-paren">(</span><em>in_size</em>, <em>out_size</em>, <em>lateral_init=None</em>, <em>upward_init=None</em>, <em>bias_init=0</em>, <em>forget_bias_init=1</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/connection/lstm.html#StatelessLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.StatelessLSTM" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Stateless LSTM layer.</p>
<p>This is a fully-connected LSTM layer as a chain. Unlike the
<a class="reference internal" href="functions.html#chainer.functions.lstm" title="chainer.functions.lstm"><code class="xref py py-func docutils literal"><span class="pre">lstm()</span></code></a> function, this chain holds upward and
lateral connections as child links. This link doesnât keep cell and
hidden states.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vectors. If <code class="docutils literal"><span class="pre">None</span></code>, parameter
initialization will be deferred until the first forward data pass
at which time the size will be determined.</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimensionality of output vectors.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>upward</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>chainer.links.Linear</em></a>) â Linear layer of upward connections.</li>
<li><strong>lateral</strong> (<a class="reference internal" href="#chainer.links.Linear" title="chainer.links.Linear"><em>chainer.links.Linear</em></a>) â Linear layer of lateral connections.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="activation-loss-normalization-functions-with-parameters">
<h2>Activation/loss/normalization functions with parameters<a class="headerlink" href="#activation-loss-normalization-functions-with-parameters" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="batchnormalization">
<h3>BatchNormalization<a class="headerlink" href="#batchnormalization" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.BatchNormalization">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">BatchNormalization</code><span class="sig-paren">(</span><em>size</em>, <em>decay=0.9</em>, <em>eps=2e-05</em>, <em>dtype=&lt;type 'numpy.float32'&gt;</em>, <em>use_gamma=True</em>, <em>use_beta=True</em>, <em>initial_gamma=None</em>, <em>initial_beta=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/normalization/batch_normalization.html#BatchNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.BatchNormalization" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Batch normalization layer on outputs of linear or convolution functions.</p>
<p>This link wraps the <a class="reference internal" href="functions.html#chainer.functions.batch_normalization" title="chainer.functions.batch_normalization"><code class="xref py py-func docutils literal"><span class="pre">batch_normalization()</span></code></a> and
<a class="reference internal" href="functions.html#chainer.functions.fixed_batch_normalization" title="chainer.functions.fixed_batch_normalization"><code class="xref py py-func docutils literal"><span class="pre">fixed_batch_normalization()</span></code></a> functions.</p>
<p>It runs in three modes: training mode, fine-tuning mode, and testing mode.</p>
<p>In training mode, it normalizes the input by <em>batch statistics</em>. It also
maintains approximated population statistics by moving averages, which can
be used for instant evaluation in testing mode.</p>
<p>In fine-tuning mode, it accumulates the input to compute <em>population
statistics</em>. In order to correctly compute the population statistics, a
user must use this mode to feed mini-batches running through whole training
dataset.</p>
<p>In testing mode, it uses pre-computed population statistics to normalize
the input variable. The population statistics is approximated if it is
computed by training mode, or accurate if it is correctly computed by
fine-tuning mode.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a><em> or </em><em>tuple of ints</em>) â Size (or shape) of channel
dimensions.</li>
<li><strong>decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Decay rate of moving average. It is used on training.</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Epsilon value for numerical stability.</li>
<li><strong>dtype</strong> (<a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dtype.html#numpy.dtype" title="(in NumPy v1.13)"><em>numpy.dtype</em></a>) â Type to use in computing.</li>
<li><strong>use_gamma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, use scaling parameter. Otherwise, use
unit(1) which makes no effect.</li>
<li><strong>use_beta</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, use shifting parameter. Otherwise, use
unit(0) which makes no effect.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>See: <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing          Internal Covariate Shift</a></p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.batch_normalization" title="chainer.functions.batch_normalization"><code class="xref py py-func docutils literal"><span class="pre">batch_normalization()</span></code></a>,
<a class="reference internal" href="functions.html#chainer.functions.fixed_batch_normalization" title="chainer.functions.fixed_batch_normalization"><code class="xref py py-func docutils literal"><span class="pre">fixed_batch_normalization()</span></code></a></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>gamma</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Scaling parameter.</li>
<li><strong>beta</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Shifting parameter.</li>
<li><strong>avg_mean</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Population mean.</li>
<li><strong>avg_var</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Population variance.</li>
<li><strong>N</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Count of batches given for fine-tuning.</li>
<li><strong>decay</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Decay rate of moving average. It is used on training.</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Epsilon value for numerical stability. This value is added
to the batch variances.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="chainer.links.BatchNormalization.start_finetuning">
<code class="descname">start_finetuning</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/normalization/batch_normalization.html#BatchNormalization.start_finetuning"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.BatchNormalization.start_finetuning" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Resets the population count for collecting population statistics.</p>
<p>This method can be skipped if it is the first time to use the
fine-tuning mode. Otherwise, this method should be called before
starting the fine-tuning mode again.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="layernormalization">
<h3>LayerNormalization<a class="headerlink" href="#layernormalization" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.LayerNormalization">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">LayerNormalization</code><span class="sig-paren">(</span><em>size=None</em>, <em>eps=1e-06</em>, <em>initial_gamma=None</em>, <em>initial_beta=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/normalization/layer_normalization.html#LayerNormalization"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.LayerNormalization" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Layer normalization layer on outputs of linear functions.</p>
<p>This link implements a âlayer normalizationâ layer
which normalizes the input units by statistics
that are computed along the second axis,
scales and shifts them.
Parameter initialization will be deferred until
the first forward data pass at which time the size will be determined.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Size of input units. If <code class="docutils literal"><span class="pre">None</span></code>, parameter initialization
will be deferred until the first forward data pass at which time
the size will be determined.</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Epsilon value for numerical stability of normalization.</li>
<li><strong>initial_gamma</strong> (<em>Initializer</em>) â Initializer for scaling vector.
If <code class="docutils literal"><span class="pre">None</span></code>, then the vector is filled by 1.
If a scalar, the vector is filled by it.
If <code class="docutils literal"><span class="pre">numpy.ndarray</span></code>, the vector is set by it.</li>
<li><strong>initial_beta</strong> (<em>Initializer</em>) â Initializer for shifting vector.
If <code class="docutils literal"><span class="pre">None</span></code>, then the vector is filled by 0.
If a scalar, the vector is filled by it.
If <code class="docutils literal"><span class="pre">numpy.ndarray</span></code>, the vector is set by it.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>gamma</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Scaling parameter.</li>
<li><strong>beta</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Shifting parameter.</li>
<li><strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Epsilon value for numerical stability.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>See: <a class="reference external" href="https://arxiv.org/abs/1607.06450">Layer Normalization</a></p>
</dd></dl>

</div>
<div class="section" id="binaryhierarchicalsoftmax">
<h3>BinaryHierarchicalSoftmax<a class="headerlink" href="#binaryhierarchicalsoftmax" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.BinaryHierarchicalSoftmax">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">BinaryHierarchicalSoftmax</code><span class="sig-paren">(</span><em>in_size</em>, <em>tree</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/loss/hierarchical_softmax.html#BinaryHierarchicalSoftmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.BinaryHierarchicalSoftmax" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Hierarchical softmax layer over binary tree.</p>
<p>In natural language applications, vocabulary size is too large to use
softmax loss.
Instead, the hierarchical softmax uses product of sigmoid functions.
It costs only <span class="math">\(O(\log(n))\)</span> time where <span class="math">\(n\)</span> is the vocabulary
size in average.</p>
<p>At first a user need to prepare a binary tree whose each leaf is
corresponding to a word in a vocabulary.
When a word <span class="math">\(x\)</span> is given, exactly one path from the root of the tree
to the leaf of the word exists.
Let <span class="math">\(\mbox{path}(x) = ((e_1, b_1), \dots, (e_m, b_m))\)</span> be the path
of <span class="math">\(x\)</span>, where <span class="math">\(e_i\)</span> is an index of <span class="math">\(i\)</span>-th internal node,
and <span class="math">\(b_i \in \{-1, 1\}\)</span> indicates direction to move at
<span class="math">\(i\)</span>-th internal node (-1 is left, and 1 is right).
Then, the probability of <span class="math">\(x\)</span> is given as below:</p>
<div class="math">
\[\begin{split}P(x) &amp;= \prod_{(e_i, b_i) \in \mbox{path}(x)}P(b_i | e_i)  \\
     &amp;= \prod_{(e_i, b_i) \in \mbox{path}(x)}\sigma(b_i x^\top
        w_{e_i}),\end{split}\]</div>
<p>where <span class="math">\(\sigma(\cdot)\)</span> is a sigmoid function, and <span class="math">\(w\)</span> is a
weight matrix.</p>
<p>This function costs <span class="math">\(O(\log(n))\)</span> time as an average length of paths
is <span class="math">\(O(\log(n))\)</span>, and <span class="math">\(O(n)\)</span> memory as the number of internal
nodes equals <span class="math">\(n - 1\)</span>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vectors.</li>
<li><strong>tree</strong> â A binary tree made with tuples like <cite>((1, 2), 3)</cite>.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter matrix.</p>
</td>
</tr>
</tbody>
</table>
<p>See: Hierarchical Probabilistic Neural Network Language Model [Morin+,
AISTAT2005].</p>
<dl class="staticmethod">
<dt id="chainer.links.BinaryHierarchicalSoftmax.create_huffman_tree">
<em class="property">static </em><code class="descname">create_huffman_tree</code><span class="sig-paren">(</span><em>word_counts</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/loss/hierarchical_softmax.html#BinaryHierarchicalSoftmax.create_huffman_tree"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.BinaryHierarchicalSoftmax.create_huffman_tree" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Makes a Huffman tree from a dictionary containing word counts.</p>
<p>This method creates a binary Huffman tree, that is required for
<a class="reference internal" href="#chainer.links.BinaryHierarchicalSoftmax" title="chainer.links.BinaryHierarchicalSoftmax"><code class="xref py py-class docutils literal"><span class="pre">BinaryHierarchicalSoftmax</span></code></a>.
For example, <code class="docutils literal"><span class="pre">{0:</span> <span class="pre">8,</span> <span class="pre">1:</span> <span class="pre">5,</span> <span class="pre">2:</span> <span class="pre">6,</span> <span class="pre">3:</span> <span class="pre">4}</span></code> is converted to
<code class="docutils literal"><span class="pre">((3,</span> <span class="pre">1),</span> <span class="pre">(2,</span> <span class="pre">0))</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>word_counts</strong> (<em>dict of int key and int</em><em> or </em><em>float values</em>) â Dictionary representing counts of words.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Binary Huffman tree with tuples and keys of <code class="docutils literal"><span class="pre">word_coutns</span></code>.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="blackout">
<h3>BlackOut<a class="headerlink" href="#blackout" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.BlackOut">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">BlackOut</code><span class="sig-paren">(</span><em>in_size</em>, <em>counts</em>, <em>sample_size</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/loss/black_out.html#BlackOut"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.BlackOut" title="Permalink to this definition">Â¶</a></dt>
<dd><p>BlackOut loss layer.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.black_out" title="chainer.functions.black_out"><code class="xref py py-func docutils literal"><span class="pre">black_out()</span></code></a> for more detail.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vectors.</li>
<li><strong>counts</strong> (<em>int list</em>) â Number of each identifiers.</li>
<li><strong>sample_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of negative samples.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter matrix.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="crf1d">
<h3>CRF1d<a class="headerlink" href="#crf1d" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.CRF1d">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">CRF1d</code><span class="sig-paren">(</span><em>n_label</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/loss/crf1d.html#CRF1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.CRF1d" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Linear-chain conditional random field loss layer.</p>
<p>This link wraps the <a class="reference internal" href="functions.html#chainer.functions.crf1d" title="chainer.functions.crf1d"><code class="xref py py-func docutils literal"><span class="pre">crf1d()</span></code></a> function.
It holds a transition cost matrix as a parameter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>n_label</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of labels.</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.crf1d" title="chainer.functions.crf1d"><code class="xref py py-func docutils literal"><span class="pre">crf1d()</span></code></a> for more detail.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>cost</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Transition cost parameter.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="chainer.links.CRF1d.argmax">
<code class="descname">argmax</code><span class="sig-paren">(</span><em>xs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/loss/crf1d.html#CRF1d.argmax"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.CRF1d.argmax" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Computes a state that maximizes a joint probability.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>xs</strong> (<em>list of Variable</em>) â Input vector for each label.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><dl class="docutils">
<dt>A tuple of <a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><code class="xref py py-class docutils literal"><span class="pre">Variable</span></code></a> representing each</dt>
<dd>log-likelihood and a list representing the argmax path.</dd>
</dl>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.6)">tuple</a></td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">See <code class="xref py py-func docutils literal"><span class="pre">crf1d_argmax()</span></code> for more
detail.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="prelu">
<h3>PReLU<a class="headerlink" href="#prelu" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.PReLU">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">PReLU</code><span class="sig-paren">(</span><em>shape=()</em>, <em>init=0.25</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/activation/prelu.html#PReLU"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.PReLU" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Parametric ReLU function as a link.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>shape</strong> (<em>tuple of ints</em>) â Shape of the parameter array.</li>
<li><strong>init</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Initial parameter value.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>See the paper for details: <a class="reference external" href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing     Human-Level Performance on ImageNet Classification</a>.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.prelu" title="chainer.functions.prelu"><code class="xref py py-func docutils literal"><span class="pre">chainer.functions.prelu()</span></code></a></p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Coefficient of parametric ReLU.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="maxout">
<h3>Maxout<a class="headerlink" href="#maxout" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Maxout">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Maxout</code><span class="sig-paren">(</span><em>in_size</em>, <em>out_size</em>, <em>pool_size</em>, <em>initialW=None</em>, <em>initial_bias=0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/activation/maxout.html#Maxout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Maxout" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Fully-connected maxout layer.</p>
<p>Let <code class="docutils literal"><span class="pre">M</span></code>, <code class="docutils literal"><span class="pre">P</span></code> and <code class="docutils literal"><span class="pre">N</span></code> be an input dimension, a pool size,
and an output dimension, respectively.
For an input vector <span class="math">\(x\)</span> of size <code class="docutils literal"><span class="pre">M</span></code>, it computes</p>
<div class="math">
\[Y_{i} = \mathrm{max}_{j} (W_{ij\cdot}x + b_{ij}).\]</div>
<p>Here <span class="math">\(W\)</span> is a weight tensor of shape <code class="docutils literal"><span class="pre">(M,</span> <span class="pre">P,</span> <span class="pre">N)</span></code>,
<span class="math">\(b\)</span> an  optional bias vector of shape <code class="docutils literal"><span class="pre">(M,</span> <span class="pre">P)</span></code>
and <span class="math">\(W_{ij\cdot}\)</span> is a sub-vector extracted from
<span class="math">\(W\)</span> by fixing first and second dimensions to
<span class="math">\(i\)</span> and <span class="math">\(j\)</span>, respectively.
Minibatch dimension is omitted in the above equation.</p>
<p>As for the actual implementation, this chain has a
Linear link with a <code class="docutils literal"><span class="pre">(M</span> <span class="pre">*</span> <span class="pre">P,</span> <span class="pre">N)</span></code> weight matrix and
an optional <code class="docutils literal"><span class="pre">M</span> <span class="pre">*</span> <span class="pre">P</span></code> dimensional bias vector.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vectors.</li>
<li><strong>out_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of output vectors.</li>
<li><strong>pool_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of channels.</li>
<li><strong>initialW</strong> (<em>3-D array</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.6)"><em>None</em></a>) â Initial weight value.
If <code class="docutils literal"><span class="pre">None</span></code>, the default initializer is used
to initialize the weight matrix.</li>
<li><strong>initial_bias</strong> (<em>2-D array</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.6)"><em>None</em></a>) â Initial bias value.
If it is float, initial bias is filled with this value.
If <code class="docutils literal"><span class="pre">None</span></code>, bias is omitted.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><p class="first last"><a class="reference internal" href="functions.html#chainer.functions.linear" title="chainer.functions.linear"><strong>linear</strong></a> (<a class="reference internal" href="core/link.html#chainer.Link" title="chainer.Link"><em>Link</em></a>) â The Linear link that performs
affine transformation.</p>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.maxout" title="chainer.functions.maxout"><code class="xref py py-func docutils literal"><span class="pre">maxout()</span></code></a></p>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last">Goodfellow, I., Warde-farley, D., Mirza, M.,
Courville, A., &amp; Bengio, Y. (2013).
Maxout Networks. In Proceedings of the 30th International
Conference on Machine Learning (ICML-13) (pp. 1319-1327).
<a class="reference external" href="http://jmlr.org/proceedings/papers/v28/goodfellow13.html">URL</a></p>
</div>
</dd></dl>

</div>
<div class="section" id="negativesampling">
<h3>NegativeSampling<a class="headerlink" href="#negativesampling" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.NegativeSampling">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">NegativeSampling</code><span class="sig-paren">(</span><em>in_size</em>, <em>counts</em>, <em>sample_size</em>, <em>power=0.75</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/loss/negative_sampling.html#NegativeSampling"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.NegativeSampling" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Negative sampling loss layer.</p>
<p>This link wraps the <a class="reference internal" href="functions.html#chainer.functions.negative_sampling" title="chainer.functions.negative_sampling"><code class="xref py py-func docutils literal"><span class="pre">negative_sampling()</span></code></a> function.
It holds the weight matrix as a parameter. It also builds a sampler
internally given a list of word counts.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>in_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Dimension of input vectors.</li>
<li><strong>counts</strong> (<em>int list</em>) â Number of each identifiers.</li>
<li><strong>sample_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.6)"><em>int</em></a>) â Number of negative samples.</li>
<li><strong>power</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.6)"><em>float</em></a>) â Power factor <span class="math">\(\alpha\)</span>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="functions.html#chainer.functions.negative_sampling" title="chainer.functions.negative_sampling"><code class="xref py py-func docutils literal"><span class="pre">negative_sampling()</span></code></a> for more detail.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><strong>W</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Weight parameter matrix.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="machine-learning-models">
<h2>Machine learning models<a class="headerlink" href="#machine-learning-models" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="classifier">
<h3>Classifier<a class="headerlink" href="#classifier" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.Classifier">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">Classifier</code><span class="sig-paren">(</span><em>predictor</em>, <em>lossfun=&lt;function softmax_cross_entropy&gt;</em>, <em>accfun=&lt;function accuracy&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/classifier.html#Classifier"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.Classifier" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A simple classifier model.</p>
<p>This is an example of chain that wraps another chain. It computes the
loss and accuracy based on a given input/label pair.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>predictor</strong> (<a class="reference internal" href="core/link.html#chainer.Link" title="chainer.Link"><em>Link</em></a>) â Predictor network.</li>
<li><strong>lossfun</strong> (<a class="reference internal" href="function_hooks.html#module-chainer.function" title="chainer.function"><em>function</em></a>) â Loss function.</li>
<li><strong>accfun</strong> (<a class="reference internal" href="function_hooks.html#module-chainer.function" title="chainer.function"><em>function</em></a>) â Function that computes accuracy.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>predictor</strong> (<a class="reference internal" href="core/link.html#chainer.Link" title="chainer.Link"><em>Link</em></a>) â Predictor network.</li>
<li><strong>lossfun</strong> (<a class="reference internal" href="function_hooks.html#module-chainer.function" title="chainer.function"><em>function</em></a>) â Loss function.</li>
<li><strong>accfun</strong> (<a class="reference internal" href="function_hooks.html#module-chainer.function" title="chainer.function"><em>function</em></a>) â Function that computes accuracy.</li>
<li><strong>y</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Prediction for the last minibatch.</li>
<li><strong>loss</strong> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Loss value for the last minibatch.</li>
<li><a class="reference internal" href="functions.html#chainer.functions.accuracy" title="chainer.functions.accuracy"><strong>accuracy</strong></a> (<a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable"><em>Variable</em></a>) â Accuracy for the last minibatch.</li>
<li><strong>compute_accuracy</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, compute accuracy on the forward
computation. The default value is <code class="docutils literal"><span class="pre">True</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="pre-trained-models">
<h2>Pre-trained models<a class="headerlink" href="#pre-trained-models" title="Permalink to this headline">Â¶</a></h2>
<p>Pre-trained models are mainly used to achieve a good performance with a small
dataset, or extract a semantic feature vector. Although <code class="docutils literal"><span class="pre">CaffeFunction</span></code>
automatically loads a pre-trained model released as a caffemodel,
the following link models provide an interface for automatically converting
caffemodels, and easily extracting semantic feature vectors.</p>
<p>For example, to extract the feature vectors with <code class="docutils literal"><span class="pre">VGG16Layers</span></code>, which is
a common pre-trained model in the field of image recognition,
users need to write the following few lines:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">chainer.links</span> <span class="k">import</span> <span class="n">VGG16Layers</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="k">import</span> <span class="n">Image</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">VGG16Layers</span><span class="p">()</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;path/to/image.jpg&quot;</span><span class="p">)</span>
<span class="n">feature</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">extract</span><span class="p">([</span><span class="n">img</span><span class="p">],</span> <span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;fc7&quot;</span><span class="p">])[</span><span class="s2">&quot;fc7&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>where <code class="docutils literal"><span class="pre">fc7</span></code> denotes a layer before the last fully-connected layer.
Unlike the usual links, these classes automatically load all the
parameters from the pre-trained models during initialization.</p>
<div class="section" id="vgg16layers">
<h3>VGG16Layers<a class="headerlink" href="#vgg16layers" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.VGG16Layers">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">VGG16Layers</code><span class="sig-paren">(</span><em>pretrained_model='auto'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/vgg.html#VGG16Layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.VGG16Layers" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A pre-trained CNN model with 16 layers provided by VGG team [1].</p>
<p>During initialization, this chain model automatically downloads
the pre-trained caffemodel, convert to another chainer model,
stores it on your local directory, and initializes all the parameters
with it. This model would be useful when you want to extract a semantic
feature vector from a given image, or fine-tune the model
on a different dataset.
Note that this pre-trained model is released under Creative Commons
Attribution License.</p>
<p>If you want to manually convert the pre-trained caffemodel to a chainer
model that can be specified in the constructor,
please use <code class="docutils literal"><span class="pre">convert_caffemodel_to_npz</span></code> classmethod instead.</p>
<table class="docutils footnote" frame="void" id="id2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>K. Simonyan and A. Zisserman, <a class="reference external" href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks
for Large-Scale Image Recognition</a></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pretrained_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) â the destination of the pre-trained
chainer model serialized as a <code class="docutils literal"><span class="pre">.npz</span></code> file.
If this argument is specified as <code class="docutils literal"><span class="pre">auto</span></code>,
it automatically downloads the caffemodel from the internet.
Note that in this case the converted chainer model is stored
on <code class="docutils literal"><span class="pre">$CHAINER_DATASET_ROOT/pfnet/chainer/models</span></code> directory,
where <code class="docutils literal"><span class="pre">$CHAINER_DATASET_ROOT</span></code> is set as
<code class="docutils literal"><span class="pre">$HOME/.chainer/dataset</span></code> unless you specify another value
as a environment variable. The converted chainer model is
automatically used from the second time.
If the argument is specified as <code class="docutils literal"><span class="pre">None</span></code>, all the parameters
are not initialized by the pre-trained model, but the default
initializer used in the original paper, i.e.,
<code class="docutils literal"><span class="pre">chainer.initializers.Normal(scale=0.01)</span></code>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><strong>available_layers</strong> (<em>list of str</em>) â The list of available layer names
used by <code class="docutils literal"><span class="pre">__call__</span></code> and <code class="docutils literal"><span class="pre">extract</span></code> methods.</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="chainer.links.VGG16Layers.convert_caffemodel_to_npz">
<em class="property">classmethod </em><code class="descname">convert_caffemodel_to_npz</code><span class="sig-paren">(</span><em>path_caffemodel</em>, <em>path_npz</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/vgg.html#VGG16Layers.convert_caffemodel_to_npz"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.VGG16Layers.convert_caffemodel_to_npz" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Converts a pre-trained caffemodel to a chainer model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path_caffemodel</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) â Path of the pre-trained caffemodel.</li>
<li><strong>path_npz</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) â Path of the converted chainer model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chainer.links.VGG16Layers.extract">
<code class="descname">extract</code><span class="sig-paren">(</span><em>images, layers=['fc7'], size=(224, 224)</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/vgg.html#VGG16Layers.extract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.VGG16Layers.extract" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Extracts all the feature maps of given images.</p>
<p>The difference of directly executing <code class="docutils literal"><span class="pre">__call__</span></code> is that
it directly accepts images as an input and automatically
transforms them to a proper variable. That is,
it is also interpreted as a shortcut method that implicitly calls
<code class="docutils literal"><span class="pre">prepare</span></code> and <code class="docutils literal"><span class="pre">__call__</span></code> functions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>images</strong> (<em>iterable of PIL.Image</em><em> or </em><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><em>numpy.ndarray</em></a>) â Input images.</li>
<li><strong>layers</strong> (<em>list of str</em>) â The list of layer names you want to extract.</li>
<li><strong>size</strong> (<em>pair of ints</em>) â The resolution of resized images used as
an input of CNN. All the given images are not resized
if this argument is <code class="docutils literal"><span class="pre">None</span></code>, but the resolutions of
all the images should be the same.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A directory in which
the key contains the layer name and the value contains
the corresponding feature map variable.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Dictionary of ~chainer.Variable</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chainer.links.VGG16Layers.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>images</em>, <em>oversample=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/vgg.html#VGG16Layers.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.VGG16Layers.predict" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Computes all the probabilities of given images.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>images</strong> (<em>iterable of PIL.Image</em><em> or </em><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><em>numpy.ndarray</em></a>) â Input images.</li>
<li><strong>oversample</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, it averages results across
center, corners, and mirrors. Otherwise, it uses only the
center.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Output that contains the class probabilities
of given images.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable">Variable</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="chainer.links.model.vision.vgg.prepare">
<code class="descclassname">chainer.links.model.vision.vgg.</code><code class="descname">prepare</code><span class="sig-paren">(</span><em>image</em>, <em>size=(224</em>, <em>224)</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/vgg.html#prepare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.model.vision.vgg.prepare" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Converts the given image to the numpy array for VGG models.</p>
<p>Note that you have to call this method before <code class="docutils literal"><span class="pre">__call__</span></code>
because the pre-trained vgg model requires to resize the given image,
covert the RGB to the BGR, subtract the mean,
and permute the dimensions before calling.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>image</strong> (<em>PIL.Image</em><em> or </em><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><em>numpy.ndarray</em></a>) â Input image.
If an input is <code class="docutils literal"><span class="pre">numpy.ndarray</span></code>, its shape must be
<code class="docutils literal"><span class="pre">(height,</span> <span class="pre">width)</span></code>, <code class="docutils literal"><span class="pre">(height,</span> <span class="pre">width,</span> <span class="pre">channels)</span></code>,
or <code class="docutils literal"><span class="pre">(channels,</span> <span class="pre">height,</span> <span class="pre">width)</span></code>, and
the order of the channels must be RGB.</li>
<li><strong>size</strong> (<em>pair of ints</em>) â Size of converted images.
If <code class="docutils literal"><span class="pre">None</span></code>, the given image is not resized.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The converted output array.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)">numpy.ndarray</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="resnet50layers">
<h3>ResNet50Layers<a class="headerlink" href="#resnet50layers" title="Permalink to this headline">Â¶</a></h3>
<dl class="class">
<dt id="chainer.links.ResNet50Layers">
<em class="property">class </em><code class="descclassname">chainer.links.</code><code class="descname">ResNet50Layers</code><span class="sig-paren">(</span><em>pretrained_model='auto'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/resnet.html#ResNet50Layers"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.ResNet50Layers" title="Permalink to this definition">Â¶</a></dt>
<dd><p>A pre-trained CNN model with 50 layers provided by MSRA [1].</p>
<p>When you specify the path of the pre-trained chainer model serialized as
a <code class="docutils literal"><span class="pre">.npz</span></code> file in the constructor, this chain model automatically
initializes all the parameters with it.
This model would be useful when you want to extract a semantic feature
vector per image, or fine-tune the model on a different dataset.
Note that unlike <code class="docutils literal"><span class="pre">VGG16Layers</span></code>, it does not automatically download a
pre-trained caffemodel. This caffemodel can be downloaded at
<a class="reference external" href="https://github.com/KaimingHe/deep-residual-networks">GitHub</a>.</p>
<p>If you want to manually convert the pre-trained caffemodel to a chainer
model that can be specified in the constructor,
please use <code class="docutils literal"><span class="pre">convert_caffemodel_to_npz</span></code> classmethod instead.</p>
<table class="docutils footnote" frame="void" id="id3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>K. He et. al., <a class="reference external" href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>pretrained_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) â the destination of the pre-trained
chainer model serialized as a <code class="docutils literal"><span class="pre">.npz</span></code> file.
If this argument is specified as <code class="docutils literal"><span class="pre">auto</span></code>,
it automatically loads and converts the caffemodel from
<code class="docutils literal"><span class="pre">$CHAINER_DATASET_ROOT/pfnet/chainer/models/ResNet-50-model.caffemodel</span></code>,
where <code class="docutils literal"><span class="pre">$CHAINER_DATASET_ROOT</span></code> is set as
<code class="docutils literal"><span class="pre">$HOME/.chainer/dataset</span></code> unless you specify another value
as an environment variable. Note that in this case the converted
chainer model is stored on the same directory and automatically
used from the second time.
If the argument is specified as <code class="docutils literal"><span class="pre">None</span></code>, all the parameters
are not initialized by the pre-trained model, but the default
initializer used in the original paper, i.e.,
<code class="docutils literal"><span class="pre">chainer.initializers.HeNormal(scale=1.0)</span></code>.</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><strong>available_layers</strong> (<em>list of str</em>) â The list of available layer names
used by <code class="docutils literal"><span class="pre">__call__</span></code> and <code class="docutils literal"><span class="pre">extract</span></code> methods.</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="chainer.links.ResNet50Layers.convert_caffemodel_to_npz">
<em class="property">classmethod </em><code class="descname">convert_caffemodel_to_npz</code><span class="sig-paren">(</span><em>path_caffemodel</em>, <em>path_npz</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/resnet.html#ResNet50Layers.convert_caffemodel_to_npz"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.ResNet50Layers.convert_caffemodel_to_npz" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Converts a pre-trained caffemodel to a chainer model.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>path_caffemodel</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) â Path of the pre-trained caffemodel.</li>
<li><strong>path_npz</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.6)"><em>str</em></a>) â Path of the converted chainer model.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chainer.links.ResNet50Layers.extract">
<code class="descname">extract</code><span class="sig-paren">(</span><em>images, layers=['pool5'], size=(224, 224)</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/resnet.html#ResNet50Layers.extract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.ResNet50Layers.extract" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Extracts all the feature maps of given images.</p>
<p>The difference of directly executing <code class="docutils literal"><span class="pre">__call__</span></code> is that
it directly accepts images as an input and automatically
transforms them to a proper variable. That is,
it is also interpreted as a shortcut method that implicitly calls
<code class="docutils literal"><span class="pre">prepare</span></code> and <code class="docutils literal"><span class="pre">__call__</span></code> functions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>images</strong> (<em>iterable of PIL.Image</em><em> or </em><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><em>numpy.ndarray</em></a>) â Input images.</li>
<li><strong>layers</strong> (<em>list of str</em>) â The list of layer names you want to extract.</li>
<li><strong>size</strong> (<em>pair of ints</em>) â The resolution of resized images used as
an input of CNN. All the given images are not resized
if this argument is <code class="docutils literal"><span class="pre">None</span></code>, but the resolutions of
all the images should be the same.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">A directory in which
the key contains the layer name and the value contains
the corresponding feature map variable.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">Dictionary of ~chainer.Variable</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="chainer.links.ResNet50Layers.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>images</em>, <em>oversample=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/resnet.html#ResNet50Layers.predict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.ResNet50Layers.predict" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Computes all the probabilities of given images.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>images</strong> (<em>iterable of PIL.Image</em><em> or </em><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><em>numpy.ndarray</em></a>) â Input images.</li>
<li><strong>oversample</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.6)"><em>bool</em></a>) â If <code class="docutils literal"><span class="pre">True</span></code>, it averages results across
center, corners, and mirrors. Otherwise, it uses only the
center.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Output that contains the class probabilities
of given images.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference internal" href="core/variable.html#chainer.Variable" title="chainer.Variable">Variable</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="chainer.links.model.vision.resnet.prepare">
<code class="descclassname">chainer.links.model.vision.resnet.</code><code class="descname">prepare</code><span class="sig-paren">(</span><em>image</em>, <em>size=(224</em>, <em>224)</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/chainer/links/model/vision/resnet.html#prepare"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#chainer.links.model.vision.resnet.prepare" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Converts the given image to the numpy array for ResNets.</p>
<p>Note that you have to call this method before <code class="docutils literal"><span class="pre">__call__</span></code>
because the pre-trained resnet model requires to resize the given
image, covert the RGB to the BGR, subtract the mean,
and permute the dimensions before calling.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>image</strong> (<em>PIL.Image</em><em> or </em><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)"><em>numpy.ndarray</em></a>) â Input image.
If an input is <code class="docutils literal"><span class="pre">numpy.ndarray</span></code>, its shape must be
<code class="docutils literal"><span class="pre">(height,</span> <span class="pre">width)</span></code>, <code class="docutils literal"><span class="pre">(height,</span> <span class="pre">width,</span> <span class="pre">channels)</span></code>,
or <code class="docutils literal"><span class="pre">(channels,</span> <span class="pre">height,</span> <span class="pre">width)</span></code>, and
the order of the channels must be RGB.</li>
<li><strong>size</strong> (<em>pair of ints</em>) â Size of converted images.
If <code class="docutils literal"><span class="pre">None</span></code>, the given image is not resized.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">The converted output array.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.13)">numpy.ndarray</a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="optimizers.html" class="btn btn-neutral float-right" title="Optimizers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="functions.html" class="btn btn-neutral" title="Standard Function implementations" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2015, Preferred Networks, inc. and Preferred Infrastructure, inc..

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'2.0.0b1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>